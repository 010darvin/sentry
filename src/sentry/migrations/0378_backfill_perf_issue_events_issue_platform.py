# Generated by Django 2.2.28 on 2023-02-14 19:42
import csv
import typing
import uuid
from collections import UserDict, defaultdict
from datetime import datetime, timedelta
from typing import Any, Mapping, Optional, Sequence, Tuple

from django.db import migrations
from snuba_sdk import Column, Condition, Entity, Function, Limit, Offset, Op, Query, Request

from sentry import eventtypes
from sentry.event_manager import GroupInfo
from sentry.eventstore.models import Event
from sentry.issues.grouptype import (
    PerformanceConsecutiveDBQueriesGroupType,
    PerformanceFileIOMainThreadGroupType,
    PerformanceMNPlusOneDBQueriesGroupType,
    PerformanceNPlusOneAPICallsGroupType,
    PerformanceNPlusOneGroupType,
    PerformanceRenderBlockingAssetSpanGroupType,
    PerformanceSlowDBQueryGroupType,
    PerformanceUncompressedAssetsGroupType,
)
from sentry.issues.ingest import process_occurrence_data, send_issue_occurrence_to_eventstream
from sentry.issues.issue_occurrence import IssueOccurrence, IssueOccurrenceData
from sentry.issues.occurrence_consumer import lookup_event
from sentry.new_migrations.migrations import CheckedMigration
from sentry.snuba.dataset import Dataset, EntityKey

if typing.TYPE_CHECKING:
    from sentry.models import Group

MIGRATION_NAME = "0378_backfill_perf_issue_events_issue_platform"


# TODO: figure out how to parameterize this to the RunPython call
DRY_RUN = False
WRITE_TO_FILE = False

START_DATETIME = datetime(2008, 5, 8)
END_DATETIME = datetime.now() + timedelta(days=1)

# TODO: update this to the appropriate date when we actually run this script
#       this is important to set to avoid unnecessarily scanning the full 90 retention period
#       when there's no data before this date
ISSUE_PLATFORM_INGEST_START_DATETIME = datetime(2023, 3, 13) - timedelta(days=1)

PROGRESS_SUCCESS_PATH_PREFIX = "success"
PROGRESS_ERROR_PATH_PREFIX = "error"

PERFORMANCE_TYPES = (
    PerformanceSlowDBQueryGroupType.type_id,
    PerformanceRenderBlockingAssetSpanGroupType.type_id,
    PerformanceNPlusOneGroupType.type_id,
    PerformanceConsecutiveDBQueriesGroupType.type_id,
    PerformanceFileIOMainThreadGroupType.type_id,
    PerformanceNPlusOneAPICallsGroupType.type_id,
    PerformanceMNPlusOneDBQueriesGroupType.type_id,
    PerformanceUncompressedAssetsGroupType.type_id,
)


class TransactionRow(typing.TypedDict):
    project_id: int
    group_id: int
    event_id: str


class BackfillEventSuccess(typing.Protocol):
    def __call__(self, created_group: "Group", mapped_occurrence: IssueOccurrenceData) -> None:
        pass


class BackfillEventError(typing.Protocol):
    def __call__(
        self,
        attempted_row: TransactionRow,
        exc: Exception,
        occurrence_nodestore_saved: bool,
        occurrence_eventstream_sent: bool,
    ) -> None:
        pass


def print_success(created_group: "Group", mapped_occurrence: IssueOccurrenceData) -> None:
    print(  # noqa: S002
        f"project_id={created_group.project_id}, group_id={created_group.id}, event_id={mapped_occurrence['event_id']}, occurrence_id={mapped_occurrence['id']}, fingerprint={mapped_occurrence['fingerprint']}"
    )


def print_exception_on_error(
    attempted_row: TransactionRow,
    exc: Exception,
    occurrence_nodestore_saved: bool,
    occurrence_eventstream_sent: bool,
) -> None:
    print(  # noqa: S002
        f"row={attempted_row}, exception={exc}, nodestore_saved={occurrence_nodestore_saved}, eventstream_sent={occurrence_eventstream_sent}"
    )


def backfill_eventstream(apps: Any, schema_editor: Any) -> None:
    """
    Backfills Performance-issue events from the transaction table to the IssuePlatform dataset(search_issues).
    1. Source Groups from postgres as the 'source of truth' for issues.
    2. For each project with a performance issue:
         a. Query snuba for the transactions backing a perf issue. We only need (project_id, group_id, event_id).
            This is paginated with a limit of 10000 transactions per page to avoid pull in too much data into memory.
         b. Query snuba for generic events backing a perf issue (we'll be dual-writing to both tables, so we need
            to ensure the event doesn't already exist in search_issues).
         c. For each transaction:
              i. Retrieve the Group row from postgres.
              ii. Retrieve the GroupHash row from postgres.
              iii. Retrieve the entire raw Transaction from node store.
              iv.
              v. Map the Transaction event to an instance of IssueOccurrenceData.
              v. Save the IssueOccurrence to node store. Normally this step would also save the appropriate models to
                 postgres like Group, GroupHash, GroupEnvironment, GroupRelease, and increment Release counts, but
                 we aren't doing that on this backfill since they should already exist when the transaction was already
                 ingested through the old style of performance issue creation.
              vi. Send the IssueOccurrence through the eventstream to be saved in search_issues.
         c. Repeat step (a), until the paginated snuba query returns no data.
    """

    Group = apps.get_model("sentry", "Group")
    GroupHash = apps.get_model("sentry", "GroupHash")

    projects_with_perf_issues = (
        Group.objects.filter(type__in=PERFORMANCE_TYPES)
        .values("project_id")
        .order_by("project_id")
        .distinct()
    )

    for project_perf_issue in projects_with_perf_issues:
        backfill_by_project(project_perf_issue["project_id"], Group, GroupHash, DRY_RUN)


class SuccessProgress(typing.TypedDict):
    occurrence_id: str
    event_id: str
    fingerprint: str


class ErrorProgress(typing.TypedDict):
    event_id: str
    exception: str
    nodestore_saved: bool
    eventstream_sent: bool


class ProjectGroupTotalProgress:
    """
    1 success row ~= 150 bytes
    1 error row ~= 150 bytes

    The performance issue with the most events is around 9M transactions. So the largest file for a group should
    be capped at around 1.28 GB ...
    """

    success: typing.MutableMapping[str, SuccessProgress]  # key: event_id
    error: typing.MutableMapping[str, ErrorProgress]  # key: event_id
    success_file: typing.TextIO
    error_file: typing.TextIO

    SUCCESS_FIELD_NAMES: Sequence[str] = ["occurrence_id", "event_id", "fingerprint"]
    ERROR_FIELD_NAMES: Sequence[str] = [
        "event_id",
        "exception",
        "nodestore_saved",
        "eventstream_sent",
    ]

    def __init__(self, project_id: int, group_id: int) -> None:
        self.project_id = project_id
        self.group_id = group_id
        self.success = {}
        self.error = {}

    def __enter__(self) -> "ProjectGroupTotalProgress":
        self.success_file = self.__get_file_handle(
            self.project_id, PROGRESS_SUCCESS_PATH_PREFIX, self.group_id
        )
        self.error_file = self.__get_file_handle(
            self.project_id, PROGRESS_ERROR_PATH_PREFIX, self.group_id
        )

        self.__parse_success_file()
        self.__parse_error_file()

        return self

    def __exit__(self) -> None:
        self.close()

    def is_processed(self, event_id: str) -> Tuple[bool, bool]:
        return event_id in self.success, event_id in self.error

    def close(self) -> None:
        try:
            self.success_file.close()
            self.error_file.close()
        except ValueError:
            pass

    def save_success_to_file(
        self, created_group: "Group", mapped_occurrence: IssueOccurrenceData
    ) -> None:
        writer = csv.DictWriter(self.success_file, fieldnames=self.SUCCESS_FIELD_NAMES)
        writer.writerow(
            {
                self.SUCCESS_FIELD_NAMES[0]: mapped_occurrence["id"],
                self.SUCCESS_FIELD_NAMES[1]: mapped_occurrence["event_id"],
                self.SUCCESS_FIELD_NAMES[2]: mapped_occurrence["fingerprint"][0],
            }
        )

    def save_error_to_file(
        self,
        attempted_row: TransactionRow,
        exc: Exception,
        occurrence_nodestore_saved: bool,
        occurrence_eventstream_sent: bool,
    ) -> None:
        writer = csv.DictWriter(self.error_file, fieldnames=self.ERROR_FIELD_NAMES)
        writer.writerow(
            {
                self.ERROR_FIELD_NAMES[0]: attempted_row["event_id"],
                self.ERROR_FIELD_NAMES[1]: type(exc).__name__,
                # save a little disk space by serializing the bool as a 1/0
                self.ERROR_FIELD_NAMES[2]: "1" if occurrence_nodestore_saved is True else "0",
                self.ERROR_FIELD_NAMES[3]: "1" if occurrence_eventstream_sent is True else "0",
            }
        )

    def __parse_success_file(self) -> None:
        reader = csv.DictReader(self.success_file, fieldnames=self.SUCCESS_FIELD_NAMES)
        for row in reader:
            self.success[row["event_id"]] = {
                "occurrence_id": row[self.SUCCESS_FIELD_NAMES[0]],
                "event_id": row[self.SUCCESS_FIELD_NAMES[1]],
                "fingerprint": row[self.SUCCESS_FIELD_NAMES[2]],
            }
        self.success_file.close()

    def __parse_error_file(self) -> None:
        reader = csv.DictReader(self.error_file, fieldnames=self.ERROR_FIELD_NAMES)
        for row in reader:
            self.error[row["event_id"]] = {
                "event_id": row[self.ERROR_FIELD_NAMES[0]],
                "exception": row[self.ERROR_FIELD_NAMES[1]],
                "nodestore_saved": True if row[self.ERROR_FIELD_NAMES[2]] == "1" else False,
                "eventstream_sent": True if row[self.ERROR_FIELD_NAMES[3]] == "1" else False,
            }
        self.error_file.close()

    def __get_file_path(self, project_id: int, path_suffix: str, group_id: int) -> str:
        from pathlib import Path

        # TODO: need to verify the relative path
        path = f"{MIGRATION_NAME}/{path_suffix}/{project_id}"
        Path(path).mkdir(parents=True, exist_ok=True)

        return f"{path}/{group_id}.csv"

    def __get_file_handle(self, project_id: int, path_suffix: str, group_id: int) -> typing.TextIO:
        # TODO: fix this for reads
        return open(self.__get_file_path(project_id, path_suffix, group_id), "a")


class FileBackedProjectProgress(UserDict[int, ProjectGroupTotalProgress]):
    """
    Holds the file handles for maintaining the progress of migrations for a project. Hopefully, we shouldn't hit
    the OS limit on open handles since the project with the most performance issues is around 9k. So this class
    should only be holding at most 2 * 9k file handles before this migration moves on to the next project.

    """

    def __init__(self, project_id: int) -> None:
        super().__init__()
        self.project_id = project_id

    def __enter__(self) -> "FileBackedProjectProgress":
        return self

    def __exit__(self, *args: Any, **kwargs: Any) -> None:
        try:
            for progress in self.values():
                progress.close()
        except ValueError:
            pass

    def get_or_init(self, group_id: int) -> ProjectGroupTotalProgress:
        if self.get(group_id):
            return self[group_id]
        else:
            progress = ProjectGroupTotalProgress(self.project_id, group_id)
            self[group_id] = progress
            return progress


def backfill_by_project(project_id: int, Group: Any, GroupHash: Any, dry_run: bool) -> None:
    next_offset: Optional[int] = 0

    already_processed_events: Mapping[int, typing.Set[str]] = defaultdict(
        set
    )  # group_id: Set[event_id]

    # it's probably ok to query all the events here instead of paginating since we should be running this script
    # closely after we cut-over
    for issue_platform_events in _query_issue_platform_events(
        project_id, start=ISSUE_PLATFORM_INGEST_START_DATETIME, end=END_DATETIME
    ):
        already_processed_events[issue_platform_events["group_id"]].add(
            issue_platform_events["event_id"]
        )

    with FileBackedProjectProgress(project_id) as previous_progress:
        while next_offset is not None:
            rows, next_offset = _query_performance_issue_events(
                project_id=project_id,
                start=START_DATETIME,
                end=END_DATETIME,
                offset=next_offset,
            )

            for row in rows:
                group_id = row["group_id"]
                event_id = row["event_id"]

                # avoid re-ingesting the same transaction since we'll be dual-writing
                if event_id not in already_processed_events[group_id]:
                    group_progress: ProjectGroupTotalProgress = previous_progress.get_or_init(
                        group_id
                    )
                    processed = group_progress.is_processed(event_id)
                    previously_succeeded = processed[0]
                    previously_errored = processed[1]

                    if (
                        previously_succeeded and not previously_errored
                    ):  # previous run was successful so skip it
                        continue
                    elif (
                        previously_succeeded and previously_errored
                    ):  # ??? shouldn't be possible, maybe a degenerate case
                        pass
                    elif (
                        not previously_succeeded and previously_errored
                    ):  # errored out previously, retry on this run
                        pass
                    else:  # fresh run, business as usual
                        backfill_perf_issue_occurrence(
                            row,
                            Group,
                            GroupHash,
                            print_success
                            if not WRITE_TO_FILE
                            else group_progress.save_success_to_file,
                            print_exception_on_error
                            if not WRITE_TO_FILE
                            else group_progress.save_error_to_file,
                            dry_run=dry_run,
                        )


def backfill_perf_issue_occurrence(
    row: TransactionRow,
    Group: Any,
    GroupHash: Any,
    on_success: BackfillEventSuccess = lambda *args: None,
    on_exception: BackfillEventError = lambda *args: None,
    dry_run: bool = True,
) -> None:
    # TODO: need to query from search_issues to make sure we're not submitting the same (project_id, group, event_id)
    occurrence_nodestore_saved = False
    occurrence_eventstream_sent = False

    try:
        project_id = row["project_id"]
        group_id = row["group_id"]
        event_id = row["event_id"]

        group = Group.objects.get(id=group_id)
        group_hash = GroupHash.objects.get(group_id=group_id)

        event: Event = lookup_event(project_id=project_id, event_id=event_id)

        # TODO: this data probably maps to evidence_data and/or evidence_display
        # from sentry.utils.performance_issues.performance_detection import EventPerformanceProblem
        #
        # hashes = event.get_hashes()
        # problems: Sequence[Optional[EventPerformanceProblem]] = EventPerformanceProblem.fetch_multi([(event, h) for h in hashes.hashes])

        et = eventtypes.get(group.data.get("type", "default"))()
        issue_title = et.get_title(group.data["metadata"])
        assert issue_title
        # need to map the base raw data to an issue occurrence
        # make sure this is consistent with how we plan to ingest performance issue occurrences
        occurrence_data: IssueOccurrenceData = IssueOccurrenceData(
            id=uuid.uuid4().hex,
            project_id=project_id,
            event_id=event_id,
            fingerprint=[group_hash.hash],
            issue_title=issue_title,  # TODO: verify
            subtitle=group.culprit,  # TODO: verify
            resource_id=None,
            evidence_data={},  # TODO: verify
            evidence_display=[],  # TODO: verify
            type=group.type,
            detection_time=datetime.now().timestamp(),
            level=None,
        )
        if dry_run is False:
            occurrence, group_info = __save_issue_occurrence(occurrence_data, event, group)
            occurrence_nodestore_saved = True

            send_issue_occurrence_to_eventstream(event, occurrence, group_info, skip_consume=True)
            occurrence_eventstream_sent = True

        on_success(group, occurrence_data)
    except Exception as e:
        on_exception(row, e, occurrence_nodestore_saved, occurrence_eventstream_sent)


def __save_issue_occurrence(
    occurrence_data: IssueOccurrenceData, event: Event, group: "Group"
) -> Tuple[IssueOccurrence, GroupInfo]:
    process_occurrence_data(occurrence_data)
    # Convert occurrence data to `IssueOccurrence`
    occurrence = IssueOccurrence.from_dict(occurrence_data)
    if occurrence.event_id != event.event_id:
        raise ValueError("IssueOccurrence must have the same event_id as the passed Event")
    # Note: For now we trust the project id passed along with the event. Later on we should make
    # sure that this is somehow validated.
    occurrence.save()

    # don't need to create releases or environments since they should be created already

    # synthesize a 'fake' group_info based off of existing data in postgres
    group_info: GroupInfo = GroupInfo(group=group, is_new=False, is_regression=False)

    return occurrence, group_info


def _query_performance_issue_events(
    project_id: int, start: datetime, end: datetime, offset: int = 0
) -> Tuple[Sequence[TransactionRow], Optional[int]]:
    page_limit = 10000

    snuba_request = Request(
        dataset=Dataset.Transactions.value,
        app_id="migration",
        query=Query(
            match=Entity(EntityKey.Transactions.value),
            select=[
                Function("arrayJoin", parameters=[Column("group_ids")], alias="group_id"),
                Column("project_id"),
                Column("event_id"),
            ],
            where=[
                Condition(Column("group_ids"), Op.IS_NOT_NULL),
                Condition(Column("project_id"), Op.EQ, project_id),
                Condition(Column("finish_ts"), Op.GTE, start),
                Condition(Column("finish_ts"), Op.LT, end),
            ],
            groupby=[Column("group_id"), Column("project_id"), Column("event_id")],
            # TODO: investigate whether having an orderby messes with pagination
            # orderby=[
            #     OrderBy(Column("project_id"), direction=Direction.ASC),
            #     OrderBy(Column("group_id"), direction=Direction.ASC),
            #     OrderBy(Column("event_id"), direction=Direction.ASC),
            # ],
            limit=Limit(page_limit),
            offset=Offset(offset),
        ),
    )
    from sentry.utils.snuba import raw_snql_query

    result_snql = raw_snql_query(
        snuba_request,
        referrer=f"{MIGRATION_NAME}._query_performance_issue_events",
        use_cache=False,
    )

    result_data = result_snql["data"]

    next_offset = None if not result_data else offset + page_limit

    return result_data, next_offset


def _query_issue_platform_events(
    project_id: int, start: datetime, end: datetime
) -> Sequence[Mapping[str, Any]]:
    snuba_request = Request(
        dataset=Dataset.IssuePlatform.value,
        app_id="migration",
        query=Query(
            match=Entity(EntityKey.IssuePlatform.value),
            select=[
                Column("group_id"),
                Column("event_id"),
            ],
            where=[
                Condition(Column("group_ids"), Op.IS_NOT_NULL),
                Condition(Column("project_id"), Op.EQ, project_id),
                Condition(Column("occurrence_type_id"), Op.IN, PERFORMANCE_TYPES),
                Condition(Column("timestamp"), Op.GTE, start),
                Condition(Column("timestamp"), Op.LT, end),
            ],
            groupby=[
                Column("group_id"),
                Column("project_id"),
                Column("occurrence_type_id"),
                Column("event_id"),
            ],
        ),
    )
    from sentry.utils.snuba import raw_snql_query

    result_snql = raw_snql_query(
        snuba_request,
        referrer=f"{MIGRATION_NAME}._query_performance_issue_events",
        use_cache=False,
    )

    return result_snql["data"]  # type: ignore[no-any-return]


class Migration(CheckedMigration):  # type: ignore[misc]
    # This flag is used to mark that a migration shouldn't be automatically run in production. For
    # the most part, this should only be used for operations where it's safe to run the migration
    # after your code has deployed. So this should not be used for most operations that alter the
    # schema of a table.
    # Here are some things that make sense to mark as dangerous:
    # - Large data migrations. Typically we want these to be run manually by ops so that they can
    #   be monitored and not block the deploy for a long period of time while they run.
    # - Adding indexes to large tables. Since this can take a long time, we'd generally prefer to
    #   have ops run this and not block the deploy. Note that while adding an index is a schema
    #   change, it's completely safe to run the operation after the code has deployed.
    is_dangerous = True

    dependencies = [
        ("sentry", "0377_groupedmesssage_type_individual_index"),
    ]

    operations = [
        migrations.RunPython(
            backfill_eventstream,
            reverse_code=migrations.RunPython.noop,
            hints={"tables": ["sentry_groupedmessage", "sentry_grouphash"]},
        )
    ]
