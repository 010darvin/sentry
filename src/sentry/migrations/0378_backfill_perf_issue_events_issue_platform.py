# Generated by Django 2.2.28 on 2023-02-14 19:42
import csv
import os.path
import typing
import uuid
from collections import UserDict, defaultdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Mapping, Optional, Sequence, Tuple

from django.db import migrations
from snuba_sdk import (
    Column,
    Condition,
    Direction,
    Entity,
    Function,
    Limit,
    Offset,
    Op,
    OrderBy,
    Query,
    Request,
)

from sentry import eventtypes
from sentry.event_manager import GroupInfo
from sentry.eventstore.models import Event
from sentry.issues.grouptype import (
    PerformanceConsecutiveDBQueriesGroupType,
    PerformanceFileIOMainThreadGroupType,
    PerformanceMNPlusOneDBQueriesGroupType,
    PerformanceNPlusOneAPICallsGroupType,
    PerformanceNPlusOneGroupType,
    PerformanceRenderBlockingAssetSpanGroupType,
    PerformanceSlowDBQueryGroupType,
    PerformanceUncompressedAssetsGroupType,
)
from sentry.issues.ingest import process_occurrence_data, send_issue_occurrence_to_eventstream
from sentry.issues.issue_occurrence import IssueOccurrence, IssueOccurrenceData
from sentry.issues.occurrence_consumer import lookup_event
from sentry.new_migrations.migrations import CheckedMigration
from sentry.snuba.dataset import Dataset, EntityKey

if typing.TYPE_CHECKING:
    from sentry.models import Group

MIGRATION_NAME = "0378_backfill_perf_issue_events_issue_platform"


# TODO: figure out how to parameterize this to the RunPython call
DRY_RUN = False
WRITE_TO_FILE = True

START_DATETIME = datetime(2008, 5, 8)
END_DATETIME = datetime.now() + timedelta(days=1)

# TODO: update this to the appropriate date when we actually run this script
#       this is important to set to avoid unnecessarily scanning the full 90 retention period
#       when there's no data before this date
ISSUE_PLATFORM_INGEST_START_DATETIME = datetime(2023, 3, 13) - timedelta(days=1)

PROGRESS_SUCCESS_PATH_PREFIX = "success"
PROGRESS_ERROR_PATH_PREFIX = "error"

PERFORMANCE_TYPES = (
    PerformanceSlowDBQueryGroupType.type_id,
    PerformanceRenderBlockingAssetSpanGroupType.type_id,
    PerformanceNPlusOneGroupType.type_id,
    PerformanceConsecutiveDBQueriesGroupType.type_id,
    PerformanceFileIOMainThreadGroupType.type_id,
    PerformanceNPlusOneAPICallsGroupType.type_id,
    PerformanceMNPlusOneDBQueriesGroupType.type_id,
    PerformanceUncompressedAssetsGroupType.type_id,
)


class TransactionRow(typing.TypedDict):
    project_id: int
    group_id: int
    event_id: str
    finish_ts: datetime


class BackfillEventSuccess(typing.Protocol):
    def __call__(
        self, timestamp: datetime, created_group: "Group", mapped_occurrence: IssueOccurrenceData
    ) -> None:
        pass


class BackfillEventError(typing.Protocol):
    def __call__(
        self,
        attempted_row: TransactionRow,
        exc: Exception,
        occurrence_nodestore_saved: bool,
        occurrence_eventstream_sent: bool,
    ) -> None:
        pass


def print_success(
    timestamp: datetime, created_group: "Group", mapped_occurrence: IssueOccurrenceData
) -> None:
    print(  # noqa: S002
        f"project_id={created_group.project_id}, group_id={created_group.id}, event_id={mapped_occurrence['event_id']}, occurrence_id={mapped_occurrence['id']}, fingerprint={mapped_occurrence['fingerprint']}"
    )


def print_exception_on_error(
    attempted_row: TransactionRow,
    exc: Exception,
    occurrence_nodestore_saved: bool,
    occurrence_eventstream_sent: bool,
) -> None:
    print(  # noqa: S002
        f"row={attempted_row}, exception={exc}, nodestore_saved={occurrence_nodestore_saved}, eventstream_sent={occurrence_eventstream_sent}"
    )


def backfill_eventstream(apps: Any, schema_editor: Any) -> None:
    """
    Backfills Performance-issue events from the transaction table to the IssuePlatform dataset(search_issues).
    1. Source Groups from postgres as the 'source of truth' for issues.
    2. For each project with a performance issue:
         a. Query snuba for the transactions backing a perf issue. We only need (project_id, group_id, event_id).
            This is paginated with a limit of 10000 transactions per page to avoid pull in too much data into memory.
         b. Query snuba for generic events backing a perf issue (we'll be dual-writing to both tables, so we need
            to ensure the event doesn't already exist in search_issues).
         c. For each transaction:
              i. Retrieve the Group row from postgres.
              ii. Retrieve the GroupHash row from postgres.
              iii. Retrieve the entire raw Transaction from node store.
              iv.
              v. Map the Transaction event to an instance of IssueOccurrenceData.
              v. Save the IssueOccurrence to node store. Normally this step would also save the appropriate models to
                 postgres like Group, GroupHash, GroupEnvironment, GroupRelease, and increment Release counts, but
                 we aren't doing that on this backfill since they should already exist when the transaction was already
                 ingested through the old style of performance issue creation.
              vi. Send the IssueOccurrence through the eventstream to be saved in search_issues.
         c. Repeat step (a), until the paginated snuba query returns no data.
    """

    Group = apps.get_model("sentry", "Group")
    GroupHash = apps.get_model("sentry", "GroupHash")

    projects_with_perf_issues = (
        Group.objects.filter(type__in=PERFORMANCE_TYPES)
        .values("project_id")
        .order_by("project_id")
        .distinct()
    )

    for project_perf_issue in projects_with_perf_issues:
        project_id = project_perf_issue["project_id"]

        # TODO: need to verify the relative path
        path = f"{MIGRATION_NAME}/{project_id}/"
        Path(path).mkdir(parents=True, exist_ok=True)

        project_done = f"{path}done.txt"

        if not os.path.isfile(project_done):
            backfill_by_project(project_id, Group, GroupHash, DRY_RUN)
            # we're done processing this project, create the file to log that we're done with it
            with open(project_done, "w") as _:
                pass


class SuccessProgress(typing.TypedDict):
    occurrence_id: str
    event_id: str
    fingerprint: str


class ErrorProgress(typing.TypedDict):
    event_id: str
    exception: str
    nodestore_saved: bool
    eventstream_sent: bool


class ProjectGroupTotalProgress:
    """
    1 success row ~= 150 bytes
    1 error row ~= 150 bytes

    The performance issue with the most events is around 9M transactions. So the largest file for a group should
    be capped at around 1.28 GB ...
    """

    last_success: Optional[
        Tuple[datetime, str]
    ]  # (finish_ts, event_id)   typing.MutableMapping[str, SuccessProgress]  # key: event_id
    last_error: Optional[
        Mapping[str, Any]
    ]  # (finish_ts, event_id)  typing.MutableMapping[str, ErrorProgress]  # key: event_id
    success_file: str
    error_file: str
    opened_success_file: typing.TextIO
    opened_error_file: typing.TextIO

    SUCCESS_FIELD_NAMES: Sequence[str] = ["finish_ts", "event_id", "occurrence_id", "fingerprint"]
    ERROR_FIELD_NAMES: Sequence[str] = [
        "finish_ts",
        "event_id",
        "exception",
        "nodestore_saved",
        "eventstream_sent",
    ]

    def __init__(self, project_id: int, group_id: int) -> None:
        self.project_id = project_id
        self.group_id = group_id
        self.last_success = None
        self.last_error = None
        self.success_file = self.__get_file_path(
            self.project_id, PROGRESS_SUCCESS_PATH_PREFIX, self.group_id
        )
        self.error_file = self.__get_file_path(
            self.project_id, PROGRESS_ERROR_PATH_PREFIX, self.group_id
        )
        self.__parse_success_file()
        self.__parse_error_file()

        self.opened_success_file = open(self.success_file, "w")
        self.opened_error_file = open(self.error_file, "w")

    def consume_error(self) -> Optional[Mapping[str, Any]]:
        ret_val = self.last_error
        self.clear_error()
        return ret_val

    def clear_error(self) -> None:
        self.last_error = None
        try:
            self.opened_error_file.close()
        except OSError:
            pass
        try:
            os.remove(self.error_file)
        except FileNotFoundError:
            pass

    def already_processed(self, timestamp: datetime, event_id: str) -> bool:
        if self.last_success is None and self.last_error is None:
            return False
        if self.last_error is not None:
            raise Exception("last_error must be cleared before processing more events")

        if self.last_success is not None:
            if timestamp < self.last_success[0]:
                return True
            elif timestamp == self.last_success[0]:
                return event_id >= self.last_success[1]
            else:
                return False
        else:
            return False

    def close(self) -> None:
        try:
            self.opened_success_file.close()
        except ValueError:
            pass

        try:
            self.opened_error_file.close()
        except ValueError:
            pass

    def save_success_to_file(
        self, timestamp: datetime, created_group: "Group", mapped_occurrence: IssueOccurrenceData
    ) -> None:
        self.last_success = (timestamp, mapped_occurrence["event_id"])
        success_writer = csv.DictWriter(
            self.opened_success_file, fieldnames=self.SUCCESS_FIELD_NAMES
        )
        success_writer.writerow(
            {
                self.SUCCESS_FIELD_NAMES[0]: timestamp,
                self.SUCCESS_FIELD_NAMES[1]: mapped_occurrence["event_id"],
                self.SUCCESS_FIELD_NAMES[2]: mapped_occurrence["id"],
                self.SUCCESS_FIELD_NAMES[3]: mapped_occurrence["fingerprint"][0],
            }
        )
        # self.opened_success_file.seek(0)

    def save_error_to_file(
        self,
        attempted_row: TransactionRow,
        exc: Exception,
        occurrence_nodestore_saved: bool,
        occurrence_eventstream_sent: bool,
    ) -> None:
        row = {
            self.ERROR_FIELD_NAMES[0]: attempted_row["finish_ts"],
            self.ERROR_FIELD_NAMES[1]: attempted_row["event_id"],
            self.ERROR_FIELD_NAMES[2]: type(exc).__name__,
            # save a little disk space by serializing the bool as a 1/0
            self.ERROR_FIELD_NAMES[3]: "1" if occurrence_nodestore_saved is True else "0",
            self.ERROR_FIELD_NAMES[4]: "1" if occurrence_eventstream_sent is True else "0",
        }
        self.last_error = row
        error_writer = csv.DictWriter(self.opened_error_file, fieldnames=self.ERROR_FIELD_NAMES)
        error_writer.writerow(row)
        self.opened_error_file.seek(0)

    def __parse_success_file(self) -> Optional[Tuple[datetime, str]]:
        try:
            with open(self.success_file) as f:
                reader = csv.DictReader(f, fieldnames=self.SUCCESS_FIELD_NAMES)
                last_row = None
                for row in reader:
                    last_row = (row["finish_ts"], row["event_id"])
                return last_row  # type: ignore[return-value]
        except FileNotFoundError:
            return None

    def __parse_error_file(self) -> Optional[Mapping[str, Any]]:
        try:
            with open(self.error_file) as f:
                reader = csv.DictReader(f, fieldnames=self.ERROR_FIELD_NAMES)
                last_row = None
                for row in reader:
                    last_row = {
                        "event_id": row[self.ERROR_FIELD_NAMES[0]],
                        "exception": row[self.ERROR_FIELD_NAMES[1]],
                        "nodestore_saved": True if row[self.ERROR_FIELD_NAMES[2]] == "1" else False,
                        "eventstream_sent": True
                        if row[self.ERROR_FIELD_NAMES[3]] == "1"
                        else False,
                    }
                return last_row
        except FileNotFoundError:
            return None

    def __get_file_path(self, project_id: int, path_suffix: str, group_id: int) -> str:
        # TODO: need to verify the relative path
        path = f"{MIGRATION_NAME}/{project_id}/{group_id}/"
        Path(path).mkdir(parents=True, exist_ok=True)

        return f"{path}{path_suffix}.csv"


if typing.TYPE_CHECKING:
    FileBackedProjectProgressType = UserDict[int, ProjectGroupTotalProgress]
else:
    FileBackedProjectProgressType = UserDict


class FileBackedProjectProgress(FileBackedProjectProgressType):
    """
    Holds the file handles for maintaining the progress of migrations for a project. Hopefully, we shouldn't hit
    the OS limit on open handles since the project with the most performance issues is around 9k. So this class
    should only be holding at most 2 * 9k file handles before this migration moves on to the next project.

    """

    def __init__(self, project_id: int) -> None:
        super().__init__()
        self.project_id = project_id

    def __enter__(self) -> "FileBackedProjectProgress":
        return self

    def __exit__(self, *args: Any, **kwargs: Any) -> None:
        try:
            for progress in self.values():
                progress.close()
        except ValueError:
            pass

    def get_or_init(self, group_id: int) -> ProjectGroupTotalProgress:
        if self.get(group_id):
            return self.__getitem__(group_id)
        else:
            progress = ProjectGroupTotalProgress(self.project_id, group_id)
            self.__setitem__(group_id, progress)
            return progress


def backfill_by_project(project_id: int, Group: Any, GroupHash: Any, dry_run: bool) -> None:
    next_offset: Optional[int] = 0

    # TODO: need to revisit this since we'll be inserting data to search_issues dataset as we backfill
    #        gonna need to figure out how to ensure we don't process dupes
    already_processed_events: Mapping[int, typing.Set[str]] = defaultdict(
        set
    )  # group_id: Set[event_id]

    # it's probably ok to query all the events here instead of paginating since we should be running this script
    # closely after we cut-over
    # for issue_platform_events in _query_issue_platform_events(
    #     project_id, start=ISSUE_PLATFORM_INGEST_START_DATETIME, end=END_DATETIME
    # ):
    #     already_processed_events[issue_platform_events["group_id"]].add(
    #         issue_platform_events["event_id"]
    #     )

    with FileBackedProjectProgress(project_id) as previous_progress:
        while next_offset is not None:
            rows, next_offset = _query_performance_issue_events(
                project_id=project_id,
                start=START_DATETIME,
                end=END_DATETIME,
                offset=next_offset,
            )

            for row in rows:
                group_id = row["group_id"]
                event_id = row["event_id"]
                finish_ts = row["finish_ts"]

                # avoid re-ingesting the same transaction since we'll be dual-writing
                if event_id not in already_processed_events[group_id]:
                    group_progress: ProjectGroupTotalProgress = previous_progress.get_or_init(
                        group_id
                    )
                    error_maybe = group_progress.consume_error()
                    if error_maybe is None:
                        already_processed = group_progress.already_processed(finish_ts, event_id)
                        if already_processed:
                            continue
                        else:
                            backfill_perf_issue_occurrence(
                                row,
                                Group,
                                GroupHash,
                                print_success
                                if not WRITE_TO_FILE
                                else group_progress.save_success_to_file,
                                print_exception_on_error
                                if not WRITE_TO_FILE
                                else group_progress.save_error_to_file,
                                dry_run=dry_run,
                            )

                    else:
                        # resume progress from the (finish_ts, event_id) in the last processed error
                        # last_error_finish_ts = error_maybe["finish_ts"]
                        # last_error_event_id = error_maybe["event_id"]
                        # last_error_exception = error_maybe["exception"]
                        # last_error_nodestore_saved = error_maybe["nodestore_saved"]
                        # last_error_eventstream_sent = error_maybe["eventstream_sent"]
                        # TODO: implement some sort of pagination scheme where we pass back in the finish_ts
                        pass


def backfill_perf_issue_occurrence(
    row: TransactionRow,
    Group: Any,
    GroupHash: Any,
    on_success: BackfillEventSuccess = lambda *args: None,
    on_exception: BackfillEventError = lambda *args: None,
    dry_run: bool = True,
) -> None:
    occurrence_nodestore_saved = False
    occurrence_eventstream_sent = False

    try:
        project_id = row["project_id"]
        group_id = row["group_id"]
        event_id = row["event_id"]
        finish_ts = row["finish_ts"]

        group = Group.objects.get(id=group_id)
        group_hash = GroupHash.objects.get(group_id=group_id)

        event: Event = lookup_event(project_id=project_id, event_id=event_id)

        # TODO: this data probably maps to evidence_data and/or evidence_display
        # from sentry.utils.performance_issues.performance_detection import EventPerformanceProblem
        #
        # hashes = event.get_hashes()
        # problems: Sequence[Optional[EventPerformanceProblem]] = EventPerformanceProblem.fetch_multi([(event, h) for h in hashes.hashes])

        et = eventtypes.get(group.data.get("type", "default"))()
        issue_title = et.get_title(group.data["metadata"])
        assert issue_title
        # need to map the base raw data to an issue occurrence
        # make sure this is consistent with how we plan to ingest performance issue occurrences
        occurrence_data: IssueOccurrenceData = IssueOccurrenceData(
            id=uuid.uuid4().hex,
            project_id=project_id,
            event_id=event_id,
            fingerprint=[group_hash.hash],
            issue_title=issue_title,  # TODO: verify
            subtitle=group.culprit,  # TODO: verify
            resource_id=None,
            evidence_data={},  # TODO: verify
            evidence_display=[],  # TODO: verify
            type=group.type,
            detection_time=datetime.now().timestamp(),
            level=None,
        )
        if dry_run is False:
            occurrence, group_info = __save_issue_occurrence(occurrence_data, event, group)
            occurrence_nodestore_saved = True

            send_issue_occurrence_to_eventstream(event, occurrence, group_info, skip_consume=True)
            occurrence_eventstream_sent = True

        on_success(finish_ts, group, occurrence_data)
    except Exception as e:
        on_exception(row, e, occurrence_nodestore_saved, occurrence_eventstream_sent)
        raise


def __save_issue_occurrence(
    occurrence_data: IssueOccurrenceData, event: Event, group: "Group"
) -> Tuple[IssueOccurrence, GroupInfo]:
    process_occurrence_data(occurrence_data)
    # Convert occurrence data to `IssueOccurrence`
    occurrence = IssueOccurrence.from_dict(occurrence_data)
    if occurrence.event_id != event.event_id:
        raise ValueError("IssueOccurrence must have the same event_id as the passed Event")
    # Note: For now we trust the project id passed along with the event. Later on we should make
    # sure that this is somehow validated.
    occurrence.save()

    # don't need to create releases or environments since they should be created already

    # synthesize a 'fake' group_info based off of existing data in postgres
    group_info: GroupInfo = GroupInfo(group=group, is_new=False, is_regression=False)

    return occurrence, group_info


def _query_performance_issue_events(
    project_id: int, start: datetime, end: datetime, offset: int = 0
) -> Tuple[Sequence[TransactionRow], Optional[int]]:
    page_limit = 10000

    snuba_request = Request(
        dataset=Dataset.Transactions.value,
        app_id="migration",
        query=Query(
            match=Entity(EntityKey.Transactions.value),
            select=[
                Column("project_id"),
                Function("arrayJoin", parameters=[Column("group_ids")], alias="group_id"),
                Column("event_id"),
                Column("finish_ts"),
            ],
            where=[
                Condition(Column("group_ids"), Op.IS_NOT_NULL),
                Condition(Column("project_id"), Op.EQ, project_id),
                Condition(Column("finish_ts"), Op.GTE, start),
                Condition(Column("finish_ts"), Op.LT, end),
            ],
            groupby=[
                Column("project_id"),
                Column("group_id"),
                Column("event_id"),
                Column("finish_ts"),
            ],
            orderby=[
                OrderBy(Column("finish_ts"), direction=Direction.ASC),
                OrderBy(Column("event_id"), direction=Direction.ASC),
            ],
            limit=Limit(page_limit),
            offset=Offset(offset),
        ),
    )
    from sentry.utils.snuba import raw_snql_query

    result_snql = raw_snql_query(
        snuba_request,
        referrer=f"{MIGRATION_NAME}._query_performance_issue_events",
        use_cache=False,
    )

    result_data = result_snql["data"]

    next_offset = None if not result_data else offset + page_limit

    return result_data, next_offset


def _query_issue_platform_events(
    project_id: int, start: datetime, end: datetime
) -> Sequence[Mapping[str, Any]]:
    snuba_request = Request(
        dataset=Dataset.IssuePlatform.value,
        app_id="migration",
        query=Query(
            match=Entity(EntityKey.IssuePlatform.value),
            select=[
                Column("group_id"),
                Column("event_id"),
                Column("timestamp"),
            ],
            where=[
                Condition(Column("group_id"), Op.IS_NOT_NULL),
                Condition(Column("project_id"), Op.EQ, project_id),
                Condition(Column("occurrence_type_id"), Op.IN, PERFORMANCE_TYPES),
                Condition(Column("timestamp"), Op.GTE, start),
                Condition(Column("timestamp"), Op.LT, end),
            ],
            groupby=[
                Column("group_id"),
                Column("occurrence_type_id"),
                Column("event_id"),
                Column("timestamp"),
            ],
            orderby=[
                OrderBy(Column("timestamp"), direction=Direction.ASC),
                OrderBy(Column("event_id"), direction=Direction.ASC),
            ],
        ),
    )
    from sentry.utils.snuba import raw_snql_query

    result_snql = raw_snql_query(
        snuba_request,
        referrer=f"{MIGRATION_NAME}._query_performance_issue_events",
        use_cache=False,
    )

    return result_snql["data"]  # type: ignore[no-any-return]


class Migration(CheckedMigration):  # type: ignore[misc]
    # This flag is used to mark that a migration shouldn't be automatically run in production. For
    # the most part, this should only be used for operations where it's safe to run the migration
    # after your code has deployed. So this should not be used for most operations that alter the
    # schema of a table.
    # Here are some things that make sense to mark as dangerous:
    # - Large data migrations. Typically we want these to be run manually by ops so that they can
    #   be monitored and not block the deploy for a long period of time while they run.
    # - Adding indexes to large tables. Since this can take a long time, we'd generally prefer to
    #   have ops run this and not block the deploy. Note that while adding an index is a schema
    #   change, it's completely safe to run the operation after the code has deployed.
    is_dangerous = True

    dependencies = [
        ("sentry", "0377_groupedmesssage_type_individual_index"),
    ]

    operations = [
        migrations.RunPython(
            backfill_eventstream,
            reverse_code=migrations.RunPython.noop,
            hints={"tables": ["sentry_groupedmessage", "sentry_grouphash"]},
        )
    ]
