# Generated by Django 2.2.28 on 2023-02-14 19:42
import csv
import typing
import uuid
from datetime import datetime, timedelta
from typing import Any, Mapping, Optional, Sequence, Tuple

from django.db import migrations
from snuba_sdk import Column, Condition, Entity, Function, Limit, Offset, Op, Query, Request

from sentry import eventtypes
from sentry.event_manager import GroupInfo
from sentry.eventstore.models import Event
from sentry.issues.grouptype import (
    PerformanceConsecutiveDBQueriesGroupType,
    PerformanceFileIOMainThreadGroupType,
    PerformanceMNPlusOneDBQueriesGroupType,
    PerformanceNPlusOneAPICallsGroupType,
    PerformanceNPlusOneGroupType,
    PerformanceRenderBlockingAssetSpanGroupType,
    PerformanceSlowDBQueryGroupType,
    PerformanceUncompressedAssetsGroupType,
)
from sentry.issues.ingest import process_occurrence_data, send_issue_occurrence_to_eventstream
from sentry.issues.issue_occurrence import IssueOccurrence, IssueOccurrenceData
from sentry.issues.occurrence_consumer import lookup_event
from sentry.new_migrations.migrations import CheckedMigration
from sentry.snuba.dataset import Dataset, EntityKey

if typing.TYPE_CHECKING:
    from sentry.models import Group

MIGRATION_NAME = "0372_backfill_perf_issue_events_issue_platform"

# TODO: figure out how to parameterize this to the RunPython call
DRY_RUN = False
WRITE_TO_FILE = False

START_DATETIME = datetime(2008, 5, 8)
END_DATETIME = datetime.now() + timedelta(days=1)


class TransactionRow(typing.TypedDict):
    project_id: int
    group_id: int
    event_id: str


class BackfillEventSuccess(typing.Protocol):
    def __call__(self, created_group: "Group", mapped_occurrence: IssueOccurrenceData) -> None:
        pass


class BackfillEventError(typing.Protocol):
    def __call__(
        self,
        attempted_row: TransactionRow,
        exc: Exception,
        occurrence_nodestore_saved: bool,
        occurrence_eventstream_sent: bool,
    ) -> None:
        pass


def save_success_to_file(created_group: "Group", mapped_occurrence: IssueOccurrenceData) -> None:
    write_to_file(
        created_group.project_id,
        "success",
        created_group.id,
        [mapped_occurrence["id"], mapped_occurrence["event_id"], mapped_occurrence["fingerprint"]],
    )


def save_error_to_file(
    attempted_row: Mapping[str, Any],
    exc: Exception,
    occurrence_nodestore_saved: bool,
    occurrence_eventstream_sent: bool,
) -> None:
    write_to_file(
        attempted_row["project_id"],
        "error",
        attempted_row["group_id"],
        [attempted_row["event_id"], exc, occurrence_nodestore_saved, occurrence_eventstream_sent],
    )


def write_to_file(
    project_id: int, path_suffix: str, group_id: int, data: typing.Iterable[Any]
) -> None:
    from pathlib import Path

    # TODO: optimize this to avoid opening and closing file handles repeatedly
    # TODO: need to verify the relative path
    path = f"{MIGRATION_NAME}/{path_suffix}/{project_id}"
    Path(path).mkdir(parents=True, exist_ok=True)

    with open(f"{path}/{group_id}.csv", "a") as f:
        writer = csv.writer(f)
        writer.writerow(data)


def print_success(created_group: "Group", mapped_occurrence: IssueOccurrenceData) -> None:
    print(  # noqa: S002
        f"project_id={created_group.project_id}, group_id={created_group.id}, event_id={mapped_occurrence['event_id']}, occurrence_id={mapped_occurrence['id']}, fingerprint={mapped_occurrence['fingerprint']}"
    )


def print_exception_on_error(
    attempted_row: TransactionRow,
    exc: Exception,
    occurrence_nodestore_saved: bool,
    occurrence_eventstream_sent: bool,
) -> None:
    print(  # noqa: S002
        f"row={attempted_row}, exception={exc}, nodestore_save={occurrence_nodestore_saved}, evenstream_sent={occurrence_eventstream_sent}"
    )


def backfill_eventstream(apps: Any, schema_editor: Any) -> None:
    """
    Backfills Performance-issue events from the transaction table to the IssuePlatform dataset(search_issues).
    1. Source Groups from postgres as the 'source of truth' for issues.
    2. For each project with a performance issue:
         a. Query snuba for the transactions backing a perf issue. We only need (project_id, group_id, event_id).
            This is paginated with a limit of 10000 transactions per page to avoid pull in too much data into memory.
         b. For each transaction:
              i. Retrieve the Group row from postgres.
              ii. Retrieve the GroupHash row from postgres.
              iii. Retrieve the entire raw Transaction from node store.
              iv. Map the Transaction event to an instance of IssueOccurrenceData.
              v. Save the IssueOccurrence to node store. Normally this step would also save the appropriate models to
                 postgres like Group, GroupHash, GroupEnvironment, GroupRelease, and increment Release counts, but
                 we aren't doing that on this backfill since they should already exist when the transaction was already
                 ingested through the old style of performance issue creation.
              vi. Send the IssueOccurrence through the eventstream to be saved in search_issues.
         c. Repeat step (a), until the paginated snuba query returns no data.
    """

    Group = apps.get_model("sentry", "Group")
    GroupHash = apps.get_model("sentry", "GroupHash")

    projects_with_perf_issues = (
        Group.objects.filter(
            type__in=(
                PerformanceSlowDBQueryGroupType.type_id,
                PerformanceRenderBlockingAssetSpanGroupType.type_id,
                PerformanceNPlusOneGroupType.type_id,
                PerformanceConsecutiveDBQueriesGroupType.type_id,
                PerformanceFileIOMainThreadGroupType.type_id,
                PerformanceNPlusOneAPICallsGroupType.type_id,
                PerformanceMNPlusOneDBQueriesGroupType.type_id,
                PerformanceUncompressedAssetsGroupType.type_id,
            )
        )
        .values("project_id")
        .order_by("project_id")
        .distinct()
    )

    for project_perf_issue in projects_with_perf_issues:
        backfill_by_project(project_perf_issue["project_id"], Group, GroupHash, DRY_RUN)


def backfill_by_project(project_id: int, Group: Any, GroupHash: Any, dry_run: bool) -> None:
    next_offset: Optional[int] = 0

    while next_offset is not None:
        rows, next_offset = _query_performance_issue_events(
            project_id=project_id,
            start=START_DATETIME,
            end=END_DATETIME,
            offset=next_offset,
        )

        for row in rows:
            backfill_perf_issue_occurrence(
                row,
                Group,
                GroupHash,
                print_success,
                # print_success if not WRITE_TO_FILE else save_success_to_file,
                # print_exception_on_error if not WRITE_TO_FILE else save_error_to_file,
                dry_run=dry_run,
            )


def backfill_perf_issue_occurrence(
    row: TransactionRow,
    Group: Any,
    GroupHash: Any,
    on_success: BackfillEventSuccess = lambda *args: None,
    on_exception: BackfillEventError = lambda *args: None,
    dry_run: bool = True,
) -> None:
    occurrence_nodestore_saved = False
    occurrence_eventstream_sent = False

    try:
        project_id = row["project_id"]
        group_id = row["group_id"]
        event_id = row["event_id"]

        group = Group.objects.get(id=group_id)
        group_hash = GroupHash.objects.get(group_id=group_id)

        event: Event = lookup_event(project_id=project_id, event_id=event_id)

        # TODO: this data probably maps to evidence_data and/or evidence_display
        # from sentry.utils.performance_issues.performance_detection import EventPerformanceProblem
        #
        # hashes = event.get_hashes()
        # problems: Sequence[Optional[EventPerformanceProblem]] = EventPerformanceProblem.fetch_multi([(event, h) for h in hashes.hashes])

        et = eventtypes.get(group.data.get("type", "default"))()
        issue_title = et.get_title(group.data["metadata"])
        assert issue_title
        # need to map the base raw data to an issue occurrence
        # make sure this is consistent with how we plan to ingest performance issue occurrences
        occurrence_data: IssueOccurrenceData = IssueOccurrenceData(
            id=uuid.uuid4().hex,
            project_id=project_id,
            event_id=event_id,
            fingerprint=[group_hash.hash],
            issue_title=issue_title,  # TODO: verify
            subtitle=group.culprit,  # TODO: verify
            resource_id=None,
            evidence_data={},  # TODO: verify
            evidence_display=[],  # TODO: verify
            type=group.type,
            detection_time=datetime.now().timestamp(),
            level=None,
        )
        if dry_run is False:
            occurrence, group_info = __save_issue_occurrence(occurrence_data, event, group)
            occurrence_nodestore_saved = True

            send_issue_occurrence_to_eventstream(event, occurrence, group_info, skip_consume=True)
            occurrence_eventstream_sent = True

        on_success(group, occurrence_data)
    except Exception as e:
        on_exception(row, e, occurrence_nodestore_saved, occurrence_eventstream_sent)


def __save_issue_occurrence(
    occurrence_data: IssueOccurrenceData, event: Event, group: "Group"
) -> Tuple[IssueOccurrence, GroupInfo]:
    process_occurrence_data(occurrence_data)
    # Convert occurrence data to `IssueOccurrence`
    occurrence = IssueOccurrence.from_dict(occurrence_data)
    if occurrence.event_id != event.event_id:
        raise ValueError("IssueOccurrence must have the same event_id as the passed Event")
    # Note: For now we trust the project id passed along with the event. Later on we should make
    # sure that this is somehow validated.
    occurrence.save()

    # don't need to create releases or environments since they should be created already

    # synthesize a 'fake' group_info based off of existing data in postgres
    group_info: GroupInfo = GroupInfo(group=group, is_new=False, is_regression=False)

    return occurrence, group_info


def _query_performance_issue_events(
    project_id: int, start: datetime, end: datetime, offset: int = 0
) -> Tuple[Sequence[TransactionRow], Optional[int]]:
    # TODO: need to make sure we're not querying any dupes, verify uniqueness on (project_id, group_id, event_id)
    page_limit = 10000

    snuba_request = Request(
        dataset=Dataset.Transactions.value,
        app_id="migration",
        query=Query(
            match=Entity(EntityKey.Transactions.value),
            select=[
                Function("arrayJoin", parameters=[Column("group_ids")], alias="group_id"),
                Column("project_id"),
                Column("event_id"),
            ],
            where=[
                Condition(Column("group_ids"), Op.IS_NOT_NULL),
                Condition(Column("project_id"), Op.EQ, project_id),
                Condition(Column("finish_ts"), Op.GTE, start),
                Condition(Column("finish_ts"), Op.LT, end),
            ],
            groupby=[Column("group_id"), Column("project_id"), Column("event_id")],
            # TODO: investigate whether having an orderby messes with pagination
            # orderby=[
            #     OrderBy(Column("project_id"), direction=Direction.ASC),
            #     OrderBy(Column("group_id"), direction=Direction.ASC),
            #     OrderBy(Column("event_id"), direction=Direction.ASC),
            # ],
            limit=Limit(page_limit),
            offset=Offset(offset),
        ),
    )
    from sentry.utils.snuba import raw_snql_query

    result_snql = raw_snql_query(
        snuba_request,
        referrer=f"{MIGRATION_NAME}._query_performance_issue_events",
        use_cache=False,
    )

    result_data = result_snql["data"]

    next_offset = None if not result_data else offset + page_limit

    return result_data, next_offset


class Migration(CheckedMigration):  # type: ignore[misc]
    # This flag is used to mark that a migration shouldn't be automatically run in production. For
    # the most part, this should only be used for operations where it's safe to run the migration
    # after your code has deployed. So this should not be used for most operations that alter the
    # schema of a table.
    # Here are some things that make sense to mark as dangerous:
    # - Large data migrations. Typically we want these to be run manually by ops so that they can
    #   be monitored and not block the deploy for a long period of time while they run.
    # - Adding indexes to large tables. Since this can take a long time, we'd generally prefer to
    #   have ops run this and not block the deploy. Note that while adding an index is a schema
    #   change, it's completely safe to run the operation after the code has deployed.
    is_dangerous = True

    dependencies = [
        ("sentry", "0371_monitor_make_org_slug_unique"),
    ]

    operations = [
        migrations.RunPython(
            backfill_eventstream,
            reverse_code=migrations.RunPython.noop,
            hints={"tables": ["sentry_groupedmessage", "sentry_grouphash"]},
        )
    ]
